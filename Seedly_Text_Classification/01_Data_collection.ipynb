{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Project 3 : Text classification of Real Estate vs Stock investments</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "This project aims to build a text classifier that is able to classify questions using a suitable model. The selected model should be able to correctly classify a given post to either Real Estate or Stocks investment. We will be exploring 2 vectorisers (CountVec and TFIDF) to create a feature vocabulary and evaluating 3 classifiers: Logistic Regression, Multinomial Naive Bayes and Support Vector Classifier. We would be using the Matthews Correlation Coefficient as the primary metric to determine the best model having the lowest number of misclassified posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "Launched in 2016, Seedly helps users make smarter financial decisions with its expense tracking app which allows users to sync their financial accounts and better manage their cash-flow.\n",
    "\n",
    "Over the years, we've introduced a community feature which allows users to crowdsource knowledge from peers before making a financial decision; an unbiased reviews platform for a myriad of products ranging from travel insurance to robo-advisors; as well as comparison tools for the open electricity market and SIM-only mobile plans. As the number of topics expands in line with the number of readers, there is a growing need to classify the questions and display them in the most appropiate section within the forum.\n",
    "\n",
    "The data science team @ Seedly would be solving this challenge by building an effective model to analyse and provide a binary classification. Focusing on Seedly's core offerings, we would be classifying posts between Real estate and Stocks investments. Due to the ease of extracting data through an API, we would be mining the 2 relevant subreddits to create a corpus to train our model on. We would then parse the data and lemmatize it before we create the feature vocabulary. After fitting and tuning the model, we will be evaluating the trained model on actual blind queries from the Seedly.SG discussion forums. Finally, we would be creating a web app to test out the delivery of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the URLs to the 2 subreddits\n",
    "url_fin = 'https://www.reddit.com/r/stocks.json'\n",
    "url_prop = 'https://www.reddit.com/r/realestateinvesting.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating a function to do an api call to the reddit json\n",
    "def reddit_grabber(url_to_use,csv_filename):\n",
    "    posts = []\n",
    "    after = None\n",
    "    total_post_counter = 0\n",
    "    num_of_cycles = 40\n",
    "\n",
    "    for a in range(num_of_cycles):\n",
    "        if after == None:\n",
    "            current_url = url_to_use\n",
    "        else:\n",
    "            current_url = url_to_use + '?after=' + after \n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers={'User-agent': 'orange black'})\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print('Status error: ', res.status_code)\n",
    "            break\n",
    "\n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        total_post_counter += len(current_posts)\n",
    "        print(\"Progess @ cycle\",a+1,\"/\",num_of_cycles,':',total_post_counter,\"posts\")\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "\n",
    "        if a > 0:\n",
    "            prev_posts = pd.read_csv('data/' + csv_filename +'.csv')\n",
    "            current_df = pd.DataFrame(posts)\n",
    "            pd.concat([prev_posts, current_df], axis=0).to_csv('data/' + csv_filename +'.csv', index = False)\n",
    "\n",
    "        else:\n",
    "            pd.DataFrame(posts).to_csv('data/' + csv_filename +'.csv', index = False)\n",
    "\n",
    "        #wait time before next loop     \n",
    "        rand_time = random.randint(3,9)\n",
    "        time.sleep(rand_time)\n",
    "\n",
    "        if after == None:\n",
    "            print(\"-- END OF SCRAPE --\")\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/stocks.json\n",
      "Progess @ cycle 1 / 40 : 27 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ikk5ly\n",
      "Progess @ cycle 2 / 40 : 52 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ikm4el\n",
      "Progess @ cycle 3 / 40 : 77 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ikq0r7\n",
      "Progess @ cycle 4 / 40 : 102 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ikn7jh\n",
      "Progess @ cycle 5 / 40 : 127 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ik1emy\n",
      "Progess @ cycle 6 / 40 : 152 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ikeek2\n",
      "Progess @ cycle 7 / 40 : 177 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ijvnu5\n",
      "Progess @ cycle 8 / 40 : 202 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ika3y6\n",
      "Progess @ cycle 9 / 40 : 227 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ik66f0\n",
      "Progess @ cycle 10 / 40 : 252 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ijnu0w\n",
      "Progess @ cycle 11 / 40 : 277 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ijmp44\n",
      "Progess @ cycle 12 / 40 : 302 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ijvw4u\n",
      "Progess @ cycle 13 / 40 : 327 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ij5lox\n",
      "Progess @ cycle 14 / 40 : 352 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ij1o38\n",
      "Progess @ cycle 15 / 40 : 377 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_iijyha\n",
      "Progess @ cycle 16 / 40 : 402 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_iizo6u\n",
      "Progess @ cycle 17 / 40 : 427 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_iib7wh\n",
      "Progess @ cycle 18 / 40 : 452 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ii8ktu\n",
      "Progess @ cycle 19 / 40 : 477 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ihtu4d\n",
      "Progess @ cycle 20 / 40 : 502 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ii8btk\n",
      "Progess @ cycle 21 / 40 : 527 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ii369v\n",
      "Progess @ cycle 22 / 40 : 552 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ii098p\n",
      "Progess @ cycle 23 / 40 : 577 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ihqt7g\n",
      "Progess @ cycle 24 / 40 : 602 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ihmkwl\n",
      "Progess @ cycle 25 / 40 : 627 posts\n",
      "https://www.reddit.com/r/stocks.json?after=t3_ihtu0j\n",
      "Progess @ cycle 26 / 40 : 631 posts\n",
      "-- END OF SCRAPE --\n"
     ]
    }
   ],
   "source": [
    "# Mining the stocks subreddit\n",
    "reddit_grabber(url_fin,'fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/realestateinvesting.json\n",
      "Progess @ cycle 1 / 40 : 25 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_iki91a\n",
      "Progess @ cycle 2 / 40 : 50 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ik7r2i\n",
      "Progess @ cycle 3 / 40 : 75 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ij5php\n",
      "Progess @ cycle 4 / 40 : 100 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_iiwklm\n",
      "Progess @ cycle 5 / 40 : 125 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_iidhkn\n",
      "Progess @ cycle 6 / 40 : 150 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ihvt4y\n",
      "Progess @ cycle 7 / 40 : 175 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_igyvgj\n",
      "Progess @ cycle 8 / 40 : 200 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_igjlce\n",
      "Progess @ cycle 9 / 40 : 225 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ig2ipw\n",
      "Progess @ cycle 10 / 40 : 250 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_iftxgh\n",
      "Progess @ cycle 11 / 40 : 275 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ifcnpf\n",
      "Progess @ cycle 12 / 40 : 300 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ieshhu\n",
      "Progess @ cycle 13 / 40 : 325 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_idyh40\n",
      "Progess @ cycle 14 / 40 : 350 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_idsm4z\n",
      "Progess @ cycle 15 / 40 : 375 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_idgo4r\n",
      "Progess @ cycle 16 / 40 : 400 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ic6ebx\n",
      "Progess @ cycle 17 / 40 : 425 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ic02by\n",
      "Progess @ cycle 18 / 40 : 450 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ibmu8a\n",
      "Progess @ cycle 19 / 40 : 475 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_ib59tq\n",
      "Progess @ cycle 20 / 40 : 500 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_iajszo\n",
      "Progess @ cycle 21 / 40 : 525 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i9vlwt\n",
      "Progess @ cycle 22 / 40 : 550 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i98chx\n",
      "Progess @ cycle 23 / 40 : 575 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i8ydzz\n",
      "Progess @ cycle 24 / 40 : 600 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i8aeef\n",
      "Progess @ cycle 25 / 40 : 625 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i8b575\n",
      "Progess @ cycle 26 / 40 : 650 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i7dl83\n",
      "Progess @ cycle 27 / 40 : 675 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i7d214\n",
      "Progess @ cycle 28 / 40 : 700 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i6m3uz\n",
      "Progess @ cycle 29 / 40 : 725 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i61508\n",
      "Progess @ cycle 30 / 40 : 750 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i5u6ex\n",
      "Progess @ cycle 31 / 40 : 775 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i551vy\n",
      "Progess @ cycle 32 / 40 : 800 posts\n",
      "https://www.reddit.com/r/realestateinvesting.json?after=t3_i4ip93\n",
      "Progess @ cycle 33 / 40 : 819 posts\n",
      "-- END OF SCRAPE --\n"
     ]
    }
   ],
   "source": [
    "# Mining the real estate subreddit\n",
    "reddit_grabber(url_prop,'prop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can observe that based on the frequency of mining using the API, the shape of the dataframe doesn't add up to the final number of posts scraped. \n",
    "\n",
    "We will proceed to remove the duplicate rows (using the title of each post) to clean up the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8806, 104)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/fin.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630, 104)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['title'],inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14019, 112)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data/prop.csv')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818, 112)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop_duplicates(subset=['title'],inplace=True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(630, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please use this thread to discuss your portfol...</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the daily discussion, so anything stoc...</td>\n",
       "      <td>r/Stocks Daily Discussion &amp;amp; Technicals Tue...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It shall be interesting to see how the market ...</td>\n",
       "      <td>Tesla to sell up to $5 billion in stock amid r...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zoom has added close to **$40 billion** to its...</td>\n",
       "      <td>Zoom's market cap dashes past Lowes, Phillip M...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n- TSLA: up more than 10x in the past 12 mont...</td>\n",
       "      <td>Which stocks do you think will have explosive ...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext  \\\n",
       "0  Please use this thread to discuss your portfol...   \n",
       "1  This is the daily discussion, so anything stoc...   \n",
       "2  It shall be interesting to see how the market ...   \n",
       "3  Zoom has added close to **$40 billion** to its...   \n",
       "4  \\n- TSLA: up more than 10x in the past 12 mont...   \n",
       "\n",
       "                                               title subreddit  \n",
       "0  Rate My Portfolio - r/Stocks Quarterly Thread ...    stocks  \n",
       "1  r/Stocks Daily Discussion &amp; Technicals Tue...    stocks  \n",
       "2  Tesla to sell up to $5 billion in stock amid r...    stocks  \n",
       "3  Zoom's market cap dashes past Lowes, Phillip M...    stocks  \n",
       "4  Which stocks do you think will have explosive ...    stocks  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df1 = df.loc[:,['selftext','title','subreddit']]\n",
    "print(new_df1.shape)\n",
    "new_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can observe that we have more posts under the Real estate category. To prevent an unbalanced dataset, we will drop title rows that have below 30 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[https://www.businessinsider.com/trump-admini...</td>\n",
       "      <td>The Trump administration is moving to implemen...</td>\n",
       "      <td>realestateinvesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello everyone, first time poster here. I'm lo...</td>\n",
       "      <td>Unconventional idea feedback: Investing in Sma...</td>\n",
       "      <td>realestateinvesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I know this question gets asked a lot, but I'l...</td>\n",
       "      <td>Has anyone had experience with buying land and...</td>\n",
       "      <td>realestateinvesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>let's say you're in NY or NJ or Cali would you...</td>\n",
       "      <td>would you move to a lower cost of state for re...</td>\n",
       "      <td>realestateinvesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hello, \\n\\n&amp;amp;#x200B;\\n\\nI am a relative new...</td>\n",
       "      <td>Separate Bank Account Question?</td>\n",
       "      <td>realestateinvesting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext  \\\n",
       "0   [https://www.businessinsider.com/trump-admini...   \n",
       "2  Hello everyone, first time poster here. I'm lo...   \n",
       "3  I know this question gets asked a lot, but I'l...   \n",
       "5  let's say you're in NY or NJ or Cali would you...   \n",
       "7  Hello, \\n\\n&amp;#x200B;\\n\\nI am a relative new...   \n",
       "\n",
       "                                               title            subreddit  \n",
       "0  The Trump administration is moving to implemen...  realestateinvesting  \n",
       "2  Unconventional idea feedback: Investing in Sma...  realestateinvesting  \n",
       "3  Has anyone had experience with buying land and...  realestateinvesting  \n",
       "5  would you move to a lower cost of state for re...  realestateinvesting  \n",
       "7                    Separate Bank Account Question?  realestateinvesting  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df2 = df2.loc[df2['title'].apply(lambda x: len(x) > 30),['selftext','title','subreddit']]\n",
    "print(new_df2.shape)\n",
    "new_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine text cols into one col and label encode subreddits\n",
    "We will then encode **'stocks'** posts as **1** and **'realestate'** posts as **0**. There is no specific need for a primary class in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the cols into a 'rawtext' col\n",
    "new_df1['rawtext'] = new_df1.selftext + ' ' + new_df1.title\n",
    "new_df2['rawtext'] = new_df2.selftext + ' ' + new_df2.title\n",
    "\n",
    "# label the subreddits as 1 and 0\n",
    "new_df1.subreddit = new_df1.subreddit.map({'stocks':1})\n",
    "new_df2.subreddit = new_df2.subreddit.map({'realestateinvesting':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the unwanted cols\n",
    "new_df1.drop(columns=['selftext','title'],inplace=True)\n",
    "new_df2.drop(columns=['selftext','title'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine both dataframes into one file and save it to a seperate CSV file and off to modelling\n",
    "[click here to access the modelling notebook](02_Modelling.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([new_df1,new_df2], axis=0).to_csv('data/final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
